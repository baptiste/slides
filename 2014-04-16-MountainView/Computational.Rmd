---
title: "Computational methods for mixed-effects models"
author: "Douglas Bates"
date: "2014-04-16"
output: 
    ioslides_presentation:
        incremental: true
---
```{r preliminaries,cache=FALSE,echo=FALSE,include=FALSE}
library(lattice)
library(knitr)
library(lme4)
opts_chunk$set(cache=TRUE)
options(show.signif.stars=FALSE)
```

# Model definition

## Linear predictors and *effects*

- In statistics, a *linear model* relates a *response* random variable, $\mathcal{Y}$, to observed *covariates* through a *linear predictor*, $\mathbf{X}\mathbf{\beta}$, where $\mathbf{X}$ is a *model matrix*.
$$ \mathcal{Y}\sim\mathcal{N}(\mathbf X\mathbf\beta,\sigma^2\mathbf I) $$

- In a *generalized linear model* (GLM) the mean response is a transformation, called the *link function*, of the linear predictor. For example,
$$\mathcal{Y}\sim\mathcal{Pois}(\exp(\mathbf{X\beta})) .$$

- A categorical covariate (e.g. `sex` or `item`) with $k$ levels generates $k-1$ columns in $\mathbf X$.  The corresponding elements of $\mathbf\beta$ are sometimes called the *effects* of the covariate.

- If the number of levels of the covariate is small and reproducible we use *fixed-effects* coefficients.

## Random effects

- When the levels of the covariate, (e.g. `item`) are a sample from a population of potential values we model the *random effects* as being from a *distribution*.

- The simplest case of a linear model with random effects is
$$ \mathcal{B}\sim\mathcal{N}(\mathbf{0},\sigma^2_b\mathbf{I}),\quad \mathcal{Y}|\mathcal{B}=\mathbf{b}\sim\mathcal{N}(\mathbf{X}\mathbf{\beta}+\mathbf{Z}\mathbf{b},\sigma^2\mathbf I)$$

- Here the *random-effects model matrix*, $\mathbf{Z}$, is the *indicator matrix* of the levels of the covariate.  In general the linear predictor for the conditional distribution $\mathcal{Y}|\mathcal{B}=\mathbf{b}$ becomes $\mathbf{X\beta+Z b}$ (e.g. $\mathcal{Y}|\mathcal{B}=\mathbf{b}\sim\mathcal{Pois}(\exp(\mathbf{X\beta+Z b}))$).

- A **mixed-effects model** has both fixed-effects, $\mathbf{\beta}$, and random effects, $\mathcal{B}$.

- Notice that:
    - it's actually the *levels* that are fixed or random, not the effects
    - in practice any model with random effects is a mixed-effects model.

## Formula language in `lme4` and `MixedModels`

- The [lme4](https://github.com/lme4/lme4) package for [R](http://www.r-project.org) represents the model with a formula.

- The formula language in [Julia](http://julialang.org) was based on that from `R`.  The [MixedModels](https://github.com/dmbates/MixedModels.jl) package for `Julia` uses the same formula representation as does `lme4`.

- Fixed-effects terms in the formula are described in the `lm` [help page](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html).  Random-effects terms are of the form `(f|g)` where
    - `f` is a linear model expression
    - `g` is an expression for a `factor`, called the *grouping factor*
    
- $\mathbf{\Sigma}$, the covariance matrix for the random effects, is block-diagonal with a block for each term.
    - i.e. in the unconditional distribution, $\mathcal{B}\sim\mathcal{N}(\mathbf{0},\mathbf{\Sigma})$, random effects from different terms are independent

- The model matrix $\mathbf{Z}$ is the horizontal concatenation of blocks for each term.

## Simple, scalar random effects

- A *simple, scalar* random-effects term is of the form `(1|g)`.  
- There is one random effect for each level of `g`.  Let $i_g$ be the number of levels.
- Its diagonal block in $\mathbf{\Sigma}$ is $\sigma^2_g\mathbf{I}_{i_g}$.
- Its vertical block in $\mathbf{Z}$ is the $n\times i_g$ indicator matrix for the levels of `g`.

## The `InstEval` data from ETH-Zurich {.build}

- These data are from several years of instructor evaluations at ETH-Zurich. 
    - `y` is the evaluation (larger is better)
    - `s` is the student id
    - `d` is the instructor id.
```{r InstEval}
str(InstEval)
```

## Partially crossed random effects
```{r fm1,cache=TRUE}
system.time(fm1 <- lmer(y ~ dept*service+(1|s)+(1|d),InstEval,REML=FALSE))
VarCorr(fm1)
```

## Fitting LMMs in `Julia`
```{.julia}
using DataFrames,MixedModels
inst = DataFrame(read_rda("./data/InstEval.rda")["InstEval"]);
fm1 = fit(lmm(y ~ dept*service + (1|s) + (1|d), inst))  # next slide
@time fit(lmm(y ~ dept*service + (1|s) + (1|d), inst));
 # elapsed time: 5.50303199 seconds (214254588 bytes allocated)
```

- In `Julia` the `using` directive attaches a package.
- The `read_rda` function reads a saved `R` data set.
- The default criterion for the `lmm` function in the `MixedModels` package is ML.
- The entire `MixedModels` package is written in `Julia`
- Libraries of compiled code such as `OpenBLAS`, `LAPACK` and `SuiteSparse` are used but they are part of the base system.
- More details are given in the [Insteval.ipynb](http://nbviewer.ipython.org/github/dmbates/slides/blob/master/2014-04-09-Evanston/InstEval.ipynb) notebook

## Results of model fit
```
Linear mixed model fit by maximum likelihood
 logLik: -118792.776708, deviance: 237585.553415

 Variance components:
                Variance    Std.Dev.
 s              0.105418    0.324681
 d              0.258416    0.508347
 Residual       1.384728    1.176745
 Number of obs: 73421; levels of grouping factors: 2972, 1128

  Fixed-effects parameters:
        Estimate Std.Error   z value
 [1]     3.22961  0.064053   50.4209
 [2]    0.129536  0.101294   1.27882
 [3]   -0.176751 0.0881352  -2.00545
 [4]   0.0517102 0.0817524  0.632522
 [5]   0.0347319  0.085621  0.405647
 [6]     0.14594 0.0997984   1.46235
 [7]    0.151689 0.0816897   1.85689
 [8]    0.104206  0.118751  0.877517
 [9]   0.0440401 0.0962985  0.457329
[10]   0.0517546 0.0986029  0.524879
[11]   0.0466719  0.101942  0.457828
[12]   0.0563461 0.0977925   0.57618
[13]   0.0596536  0.100233   0.59515
[14]  0.00556281  0.110867 0.0501757
[15]    0.252025 0.0686507   3.67112
[16]   -0.180757  0.123179  -1.46744
[17]   0.0186492  0.110017  0.169512
[18]   -0.282269 0.0792937  -3.55979
[19]   -0.494464 0.0790278  -6.25683
[20]   -0.392054  0.110313  -3.55403
[21]   -0.278547 0.0823727  -3.38154
[22]   -0.189526  0.111449  -1.70056
[23]   -0.499868 0.0885423  -5.64553
[24]   -0.497162 0.0917162  -5.42065
[25]    -0.24042 0.0982071   -2.4481
[26]   -0.223013 0.0890548  -2.50422
[27]   -0.516997 0.0809077  -6.38997
[28]   -0.384773  0.091843  -4.18946
```

# Computational methods

## Use of a relative covariance factor

- In general the $q\times q$ covariance matrix, $\mathbf{\Sigma}$, depends upon a small number of parameters - 2 in this case
- Because $\mathbf{\Sigma}$ is a covariance matrix it must be positive semi-definite.  
- We write $\mathbf{\Sigma}$ using its _relative Cholesky factor_, $\mathbf{\Lambda}_{\mathbf{\theta}}$, $$ \mathbf{\Sigma}=\sigma^2\mathbf{\Lambda}_{\mathbf{\theta}}\mathbf{\Lambda}_{\mathbf{\theta}}'$$ and write $\mathcal{B}$ as $$\mathcal{B}=\mathbf{\Lambda}_{\mathbf{\theta}}\mathcal{U}$$ where the unconditional distribution of $\mathcal{U}$ is $$\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})$$

## Joint density of $\mathcal{Y}$ and $\mathcal{U}$

- $$f_{\mathcal{U},\mathcal{Y}}(\mathbf{u},\mathbf{y})=\frac{1}{\left(2\pi\sigma^2\right)^{(n+q)/2}} \exp\left(\frac{||\mathbf{y-X\beta-Z\Lambda_\theta u}||^2+ ||\mathbf{u}||^2}{-2\sigma^2}\right)$$
- We can determine the _conditional mean_, $\tilde{\mathbf{u}}=\mathbb{E}(\mathcal{U}|\mathcal{Y}=\mathbf{y})$, as the solution to the penalized least squares problem
$$\tilde{\mathbf{u}}=\arg\min_{\mathbf{u}}||\mathbf{y-X\beta-Z\Lambda_\theta u}||^2+||\mathbf{u}||^2$$
for which the _normal equations_ are
$$\left(\mathbf{\Lambda'Z'Z\Lambda+I}\right)\tilde{\mathbf{u}}=\mathbf{\Lambda'Z'\left(\mathbf{y-X\beta}\right)}$$

## The sparse Cholesky factor

- To solve the normal equations we create the _sparse Cholesky factor_ $$\mathbf{LL'=P\left(\Lambda'Z'Z\Lambda+I\right)P'}$$ where $\mathbf{L}$ is a $q\times q$ sparse lower triangular matrix and $\mathbf{P}$ represents a _fill-reducing permutation_.
- $\mathbf{P}$ doesn't affect the theory but is important in practice.
- Most sparse Cholesky software allows for the determination of $\mathbf{P}$, part of the _symbolic phase_, to be performed separately from the _numeric phase_ in which the numerical values in $\mathbf{L}$ are evaluated.
- We must repeat the numeric phase for each evaluation of the objective but the symbolic phase need only be done once.
- Notice that even when $\mathbf{Z'Z}$ or $\mathbf{\Lambda}_{\mathbf{\theta}}$ are rank deficient, the matrix to be decomposed is of full rank.
- Tim Davis's [CHOLMOD](http://www.cise.ufl.edu/research/sparse/cholmod/), which allows a _supernodal_ decomposition (a sparse/dense hybrid), is used in `R` and `Julia`.

## "Profiling out" $\mathbf{\beta}$ and $\sigma^2$

- If we extend the penalized least squares problem to $$\arg\min_{\mathbf{u},\mathbf{\beta}}||\mathbf{y-X\beta-Z\Lambda_\theta u}||^2+||\mathbf{u}||^2$$ with normal equations $$\begin{bmatrix}\mathbf{\Lambda_\theta'Z'Z\Lambda_\theta+I} & \mathbf{X'Z\Lambda_\theta}\\ \mathbf{\Lambda_\theta'Z'X}&\mathbf{X'X}\end{bmatrix}\begin{bmatrix}\mathbf{\tilde{u}}\\ \mathbf{\hat{\beta}_\theta}\end{bmatrix}=\begin{bmatrix}\mathbf{\Lambda_\theta'Z'y}\\ \mathbf{X'y}\end{bmatrix}$$ to obtain the _conditional mle_, $\mathbf{\hat{\beta}_\theta}$.
- Solving this system is straightforward once $\mathbf{P}$ and $\mathbf{L_\theta}$ are determined.
- Similarly, the conditional mle, $\widehat{\sigma^2}_\theta$, can be evaluated as the minimum penalized residual sum-of-squares (PRSS), $r^2_\theta$, divided by $n$

## The profiled log-likelihood

- We can now express the _profiled_ log-likelihood, $\ell(\mathbf{\theta|y})$, on the deviance scale (negative twice the log-likelihood), as $$-2\ell(\mathbf{\theta|y})=\log\left(|\mathbf{L_\theta}|^2\right)+n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right]$$
- The parameter estimates, $\mathbf{\hat{\theta}}$, minimize the deviance.
- The other parameter estimates, $\mathbf{\hat{\beta}}$ and $\widehat{\sigma^2}$, are evaluated at $\hat{\mathbf{\theta}}$
- When minimizing $-2\ell(\mathbf{\theta|y})$ for each evaluation we must
    - Update $\mathbf\Lambda_\theta$ and $\mathbf{L_\theta}$
    - Solve the penalized least squares problem to obtain $r^2_{\mathbf{\theta}}$
- Most of the execution time is spent in updating $\mathbf{L_\theta}$ and a few matrices related to $\mathbf{X}$.
