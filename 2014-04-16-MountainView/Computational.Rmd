---
title: "Computational methods for mixed-effects models"
author: "Douglas Bates"
date: "2014-04-16"
output: 
    ioslides_presentation:
        incremental: true
---
```{r preliminaries,cache=FALSE,echo=FALSE,include=FALSE}
library(lattice)
library(knitr)
library(lme4)
opts_chunk$set(cache=TRUE)
options(show.signif.stars=FALSE)
```

# Model definition

## Linear predictors and *effects*

- In statistics, a *linear model* relates a *response* random variable, $\mathcal{Y}$, to observed *covariates* through a *linear predictor*, $\mathbf{\eta}=\mathbf{X}\mathbf{\beta}$, where $\mathbf{X}$ is an $n\times p$ *model matrix*.
$$ \mathcal{Y}\sim\mathcal{N}(\mathbf X\mathbf\beta,\sigma^2\mathbf I) $$

- In a *generalized linear model* (GLM) the mean response is a transformation, called the *link function*, of the linear predictor. For example,
$$\mathcal{Y}\sim\mathcal{Pois}(\exp(\mathbf{X\beta})) .$$

- A categorical covariate (e.g. `sex` or `item`) with $k$ levels generates $k-1$ columns in $\mathbf X$.  The corresponding elements of $\mathbf\beta$ are sometimes called the *effects* of the covariate.

- If the number of levels of the covariate is small and reproducible the effects are *fixed-effects* coefficients, i.e. some elements of $\mathbf{\beta}$.

## Random effects

- When the levels of the covariate, (e.g. `item`), are a sample from a population, the distribution of the *random effects* random variable, $\mathcal{B}$, is typically modeled as $\mathcal{B}\sim\mathcal{N}(\mathbf{0,\Sigma})$.

- A simple, "one-way" random effects model is $\mathcal{B}\sim\mathcal{N}(\mathbf{0},\sigma^2_b\mathbf{I}_q)$, $\mathcal{Y}|\mathcal{B}=\mathbf{b}\sim\mathcal{N}(\mathbf{X}\mathbf{\beta}+\mathbf{Z}\mathbf{b},\sigma^2\mathbf{I}_n)$ 
    - $\mathbf{Z}$ is the $n\times q$ *indicator matrix* of the levels of the covariate.
    - the parameters are $\mathbf{\beta}$, $\sigma^2_b$ and $\sigma^2$
    - technically, the optimal $\mathbf{b}$ is not a parameter.  We call it the *conditional mode* of the random effects

- For generalized linear mixed models (GLMMs) we have $\mathbf{\eta}=\mathbf{X\beta+Z b}$ in $\mathcal{Y}|\mathcal{B}=\mathbf{b}$
    - e.g. $\mathcal{Y}|\mathcal{B}=\mathbf{b}\sim\mathcal{Pois}(\exp(\mathbf{X\beta+Z b}))$.

## Mixed-effects 
- A **mixed-effects model** has both fixed-effects, $\mathbf{\beta}$, and random effects, $\mathcal{B}$.

- Notice that:
    - it's actually the *levels* that are fixed or random, not the effects
    - in practice any model with random effects is a mixed-effects model.

## Formula language in `lme4` and `MixedModels`

- The [lme4](https://github.com/lme4/lme4) package for [R](http://www.r-project.org) represents the model with a formula.

- The formula language in [Julia](http://julialang.org) was based on that from `R`.  The [MixedModels](https://github.com/dmbates/MixedModels.jl) package for `Julia` uses the same formula representation as does `lme4`.

- Fixed-effects terms in the formula are described in the `lm` [help page](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html).  Random-effects terms are of the form `(f|g)` where
    - `f` is a linear model expression
    - `g` is an expression for a `factor`, called the *grouping factor*
    
- $\mathbf{\Sigma}$, the covariance matrix for the random effects, is block-diagonal with a block for each term.
    - i.e. in the unconditional distribution, $\mathcal{B}\sim\mathcal{N}(\mathbf{0},\mathbf{\Sigma})$, random effects from different terms are independent

- The model matrix $\mathbf{Z}$ is the horizontal concatenation of blocks for each term.

## Simple, scalar random effects

- A *simple, scalar* random-effects term is of the form `(1|g)`.  
- There is one random effect for each level of `g`.  Let $i_g$ be the number of levels.
- Its diagonal block in $\mathbf{\Sigma}$ is $\sigma^2_g\mathbf{I}_{i_g}$.
- Its vertical block in $\mathbf{Z}$ is the $n\times i_g$ indicator matrix for the levels of `g`.

## The `InstEval` data from ETH-Zurich {.build}

- These data are from several years of instructor evaluations at ETH-Zurich. 
    - `y` is the evaluation (larger is better)
    - `s` is the student id
    - `d` is the instructor id.
```{r InstEval}
str(InstEval)
```

## Partially crossed random effects
```{r fm1,cache=TRUE}
system.time(fm1 <- lmer(y ~ dept*service+(1|s)+(1|d),InstEval,REML=FALSE))
VarCorr(fm1)
```

## Fitting LMMs in `Julia`
```{.julia}
using DataFrames,MixedModels
inst = DataFrame(read_rda("./data/InstEval.rda")["InstEval"]);
fm1 = fit(lmm(y ~ dept*service + (1|s) + (1|d), inst))  # next slide
@time fit(lmm(y ~ dept*service + (1|s) + (1|d), inst));
 # elapsed time: 5.50303199 seconds (214254588 bytes allocated)
```

- In `Julia` the `using` directive attaches a package.
- The `read_rda` function reads a saved `R` data set.
- The default criterion for the `lmm` function in the `MixedModels` package is ML.
- The entire `MixedModels` package is written in `Julia`
- Libraries of compiled code such as `OpenBLAS`, `LAPACK` and `SuiteSparse` are used but they are part of the base system.
- More details are given in the [InstEval.ipynb](http://nbviewer.ipython.org/github/dmbates/slides/blob/master/2014-04-09-Evanston/InstEval.ipynb) `IJulia` notebook

## Results of model fit
```
Linear mixed model fit by maximum likelihood
 logLik: -118792.776708, deviance: 237585.553415

 Variance components:
                Variance    Std.Dev.
### <b>
 s              0.105418    0.324681
 d              0.258416    0.508347
### </b>
 Residual       1.384728    1.176745
 Number of obs: 73421; levels of grouping factors: 2972, 1128

  Fixed-effects parameters:
        Estimate Std.Error   z value
 [1]     3.22961  0.064053   50.4209
 [2]    0.129536  0.101294   1.27882
 [3]   -0.176751 0.0881352  -2.00545
 [4]   0.0517102 0.0817524  0.632522
 [5]   0.0347319  0.085621  0.405647
 [6]     0.14594 0.0997984   1.46235
 [7]    0.151689 0.0816897   1.85689
 [8]    0.104206  0.118751  0.877517
 [9]   0.0440401 0.0962985  0.457329
[10]   0.0517546 0.0986029  0.524879
[11]   0.0466719  0.101942  0.457828
[12]   0.0563461 0.0977925   0.57618
[13]   0.0596536  0.100233   0.59515
[14]  0.00556281  0.110867 0.0501757
[15]    0.252025 0.0686507   3.67112
[16]   -0.180757  0.123179  -1.46744
[17]   0.0186492  0.110017  0.169512
[18]   -0.282269 0.0792937  -3.55979
[19]   -0.494464 0.0790278  -6.25683
[20]   -0.392054  0.110313  -3.55403
[21]   -0.278547 0.0823727  -3.38154
[22]   -0.189526  0.111449  -1.70056
[23]   -0.499868 0.0885423  -5.64553
[24]   -0.497162 0.0917162  -5.42065
[25]    -0.24042 0.0982071   -2.4481
[26]   -0.223013 0.0890548  -2.50422
[27]   -0.516997 0.0809077  -6.38997
[28]   -0.384773  0.091843  -4.18946
```

# Computational methods

## Use of a relative covariance factor

- The $q\times q$ covariance matrix, $\mathbf{\Sigma}$, depends upon a small number of parameters, 2 in this case.
- Because $\mathbf{\Sigma}$ is a covariance matrix it must be positive semi-definite.  
- We write $\mathbf{\Sigma}$ using its _relative Cholesky factor_, $\mathbf{\Lambda}_{\mathbf{\theta}}$, $$ \mathbf{\Sigma}=\sigma^2\mathbf{\Lambda}_{\mathbf{\theta}}\mathbf{\Lambda}_{\mathbf{\theta}}'$$ and write $\mathcal{B}$ as $$\mathcal{B}=\mathbf{\Lambda}_{\mathbf{\theta}}\mathcal{U}$$ where the unconditional distribution of $\mathcal{U}$ is $$\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})$$

## Joint density of $\mathcal{Y}$ and $\mathcal{U}$

$$f_{\mathcal{U},\mathcal{Y}}(\mathbf{u},\mathbf{y})=\frac{1}{\left(2\pi\sigma^2\right)^{(n+q)/2}} \exp\left(\frac{||\mathbf{y-X\beta-Z\Lambda_\theta u}||^2+ ||\mathbf{u}||^2}{-2\sigma^2}\right)$$

- We can determine the _conditional mean_, $\tilde{\mathbf{u}}_\theta=\mathbb{E}(\mathcal{U}|\mathcal{Y}=\mathbf{y})$, as the solution to the penalized least squares problem (PLS) $$\tilde{\mathbf{u}}=\arg\min_{\mathbf{u}}||\mathbf{y-X\beta-Z\Lambda_\theta u}||^2+||\mathbf{u}||^2$$ for which the _normal equations_ are $$\left(\mathbf{\Lambda'Z'Z\Lambda+I}\right)\tilde{\mathbf{u}}_\theta=\mathbf{\Lambda'Z'\left(\mathbf{y-X\beta}\right)}$$

## The sparse Cholesky factor

- To solve the normal equations we create the _sparse Cholesky factor_ $$\mathbf{LL'=P\left(\Lambda'Z'Z\Lambda+I\right)P'}$$ where $\mathbf{L}$ is a $q\times q$ sparse lower triangular matrix and $\mathbf{P}$ represents a _fill-reducing permutation_.
- $\mathbf{P}$ doesn't affect the theory but is important in practice.
- Most sparse Cholesky software allows for the determination of $\mathbf{P}$, part of the _symbolic phase_, to be performed separately from the _numeric phase_ in which the numerical values in $\mathbf{L}$ are evaluated.
- We must repeat the numeric phase for each evaluation of the objective but the symbolic phase need only be done once.
- Notice that even when $\mathbf{Z'Z}$ or $\mathbf{\Lambda}_{\mathbf{\theta}}$ are rank deficient, the matrix to be decomposed is of full rank.
- Tim Davis's [CHOLMOD](http://www.cise.ufl.edu/research/sparse/cholmod/), which allows a _supernodal_ decomposition (a sparse/dense hybrid), is used in `R` and `Julia`.

## "Profiling out" $\mathbf{\beta}$ and $\sigma^2$

- We obtain the _conditional mle_, $\mathbf{\hat{\beta}_\theta}$, by extending the PLS problem to $$\tilde{\mathbf u}_\theta, \hat{\mathbf\beta}_\theta =\arg\min_{\mathbf{u},\mathbf{\beta}}||\mathbf{y-X\beta-Z\Lambda_\theta u}||^2+||\mathbf{u}||^2$$ with normal equations $$\begin{bmatrix}\mathbf{\Lambda_\theta'Z'Z\Lambda_\theta+I} & \mathbf{X'Z\Lambda_\theta}\\ \mathbf{\Lambda_\theta'Z'X}&\mathbf{X'X}\end{bmatrix}\begin{bmatrix}\tilde{\mathbf u}_\theta\\ \mathbf{\hat{\beta}_\theta}\end{bmatrix}=\begin{bmatrix}\mathbf{\Lambda_\theta'Z'y}\\ \mathbf{X'y}\end{bmatrix}$$ 
- Solving this system is straightforward once $\mathbf{P}$ and $\mathbf{L_\theta}$ are determined.
- Similarly, the conditional mle, $\widehat{\sigma^2}_\theta$, can be evaluated as the minimum penalized residual sum-of-squares (PRSS), $r^2_\theta$, divided by $n$

## The profiled log-likelihood

- We can now express the _profiled_ log-likelihood, $\ell(\mathbf{\theta|y})$, on the deviance scale (negative twice the log-likelihood), as $$-2\ell(\mathbf{\theta|y})=\log\left(|\mathbf{L_\theta}|^2\right)+n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right]$$
- The parameter estimates, $\mathbf{\hat{\theta}}$, minimize the profiled deviance.
- The other parameter estimates, $\mathbf{\hat{\beta}}$ and $\widehat{\sigma^2}$, are evaluated at $\hat{\mathbf{\theta}}$
- When minimizing $-2\ell(\mathbf{\theta|y})$, at each objective function evaluation we must
    - Update $\mathbf\Lambda_\theta$ and $\mathbf{L_\theta}$
    - Solve the penalized least squares problem to obtain $r^2_{\mathbf{\theta}}$

## Where is the time spent?

- If we profile (in the computer science sense) the model fit (see the [IJulia notebook](http://nbviewer.ipython.org/github/dmbates/slides/2014-04-09-Evanston/InstEval.html)), the important results are
```
  6621 ....3/NLopt/src/NLopt.jl; nlopt_callback_wrapper; line: 387
    6620 ...linearmixedmodels.jl; obj; line: 13
     2321 ...dels/src/scalarn.jl; solve!; line: 72
        1934 linalg/cholmod.jl; solve; line: 943
         1791 linalg/cholmod.jl; solve; line: 793
     3261 ...dels/src/scalarn.jl; theta!; line: 108
      3235 linalg/cholmod.jl; cholfact!; line: 669
```

-  half the time for the evaluation of the objective function is spent updating the sparse Cholesky factor (`linalg/cholmod.jl; cholfact!`)
- over 1/4 of the time is spent in solving a sparse system of equations using this decomposition (`linalg/cholmod.jl; solve`)

# Fitting GLMMs

## Integral of scaled conditional density

- For GLMMs or LMMs the log-likelihood depends on the density of $\mathcal{U}|\mathcal{Y}=\mathbf{y}$
- We know this density up to a scale factor but the likelihood **is** the scale factor
- For LMMs the conditional density is Gaussian so all we need is the height at the mode ($r^2_\theta$) and the Hessian at the mode (related to $\mathbf{L_\theta L_\theta'}$).  Recall $$-2\ell(\mathbf{\theta|y})=\log\left(|\mathbf{L_\theta}|^2\right)+n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right]$$.
- For GLMMs we do not have a closed-form expression for the integral of the unscaled density.
- The high-dimensional integral can be expressed as a product of low-dimensional integrals in certain cases.
- In the interesting cases (multiple, partially-crossed grouping factors) it can't
- The only saving grace is that the integrand shape is like a multivariate Gaussian density.

## The Laplace approximation

- For fixed $\mathbf{\theta}$ and $\mathbf{\beta}$ determine the _conditional mode_, $\tilde{\mathbf u}(\mathbf{\theta,\beta})$, and the Hessian factor, $\mathbf{L_{\theta,\beta}}$
- We use _Penalized Iteratively Reweighted Least Squares_ (PIRLS) to do this.
- That is
    - solve a weighted penalized least squares (WPLS) problem for $\mathbf{\delta_u}$
    - increment $\mathbf{u}$, evaluate $\mathbf{\eta}$, $\mathbf{\mu}$ and the variance
    - update the weights and iterate
- At $\tilde{\mathbf u}$, evaluate $\mathbf{L_{\theta,\beta}}$, the GLM deviance and the penalty, $||\tilde{\mathbf u}||^2$.
- Strictly speaking we can't profile out $\mathbf{\beta}$ and it must be incorporated in the nonlinear optimizer.
- It can be worthwhile approximating further by using PIRLS to jointly optimize $\tilde{\mathbf u}$ and $\hat{\mathbf\beta}$
    - This doesn't seem to be as big a win as we hoped it would be

# Current enhancements in Julia package

## Special cases based on the form of $\mathbf{\Lambda}_\theta$ and $\mathbf{L}$

- With a single r-e term, $\mathbf{\Lambda}$ is repeated block-diagonal ($i_g$ repetitions of the same block) and $\mathbf{L}$ is block-diagonal (diagonal for a single, scalar term).
    - obvious parallelization and simplification of storage
- Multiple terms with *nested* grouping factors produces a "fractal" structure
    - block-diagonal at coarsest level
    - each inner level is block-diagonal plus a dense row of blocks
    - $\mathbf{L}$ has the same structure. (i.e. no fill-in)
    - $\mathbf{P}$ is simply based on a post-ordering
- Multiple, non-nested scalar terms
    - $\mathbf{\Lambda}$ is diagonal so pre-compute $\mathbf{Z'Z}$ and use symmetric scaling.
    - Good for LMMs, doesn't help much for GLMMs (b/c of reweighting)
    
## Exploiting indicator structure in $\mathbf{Z}$

- Vertical blocks of $\mathbf{Z}$ are indicator matrices or row-wise multiples of indicators
- Number of non-zeros per row is constant
    - store [Compressed Sparse Row](http://en.wikipedia.org/wiki/Sparse_matrix) (CSR) of $\mathbf{Z}$ or CSC of $\mathbf{Z'}$
- Don't need column-pointers in CSC (row pointers in CSR) b/c of pattern
- In Julia for the general case store a vector of row block numbers and a dense matrix of nonzeros
    - Can form $\mathbf{\Lambda_\theta'Z'}$ with dense matrix multiplication
    - Can also apply weights for $\mathbf{\Lambda_\theta'}\mathbf{Z'}\mathbf{W}\mathbf{Z}\mathbf{\Lambda_\theta}$ ($\mathbf{W}$ is pos-definite diagonal) on the dense matrix.

# Fitting GLMMs at "Google scale"

## Faster sparse Cholesky factorization

- At this point I think working on anything other than the sparse Choleksy update and solutions will not help much
    - In our experience a supernodal decomposition can help a lot
    - Latest versions of CHOLMOD (part of [Suitesparse](http://www.cise.ufl.edu/research/sparse/SuiteSparse/)) use CUBLAS for GPU support
        - Configuring and working with CHOLMOD is already difficult
        - Adding CUBLAS may be the straw that broke the camel's back
    - [Eigen](http://eigen.tuxfamily.org) provides much cleaner simplicial Cholesky
        - templated C++ linear algebra package
        - no native supernodal decomposition at present
    - Parallel sparse Cholesky implementations
        - I don't have much experience with these
        - [PETSc](http://www.mcs.anl.gov/petsc/) provides interfaces to [PaStiX](http://pastix.gforge.inria.fr) and to [MUMPS](http://mumps.enseeiht.fr/)
        - Tim Davis has a [list](https://www.cise.ufl.edu/research/sparse/codes/) linking to several packages

# Other avenues of research

## Iterative solvers

- It may be faster to use distributed iterative solvers to solve the PLS or PIRLS problems
- That is, avoid the sparse Cholesky factorization
- I'm not sure how to evaluate $|\mathbf{\Lambda_\theta'Z'Z\Lambda_\theta+I}|$ or the weighted version quickly without forming $\mathbf{L}_\theta$.

## Evaluating gradients and Hessians

- For an LMM it is possible to evaluate the gradient and Hessian of $-2\ell(\mathbf{\theta|y})$ and hence use more sophisticated optimization techniques.
- That's the good news.
- The bad news is that, except in simple cases, a gradient evaluation is much more expensive than many evaluations of the objective.
- This is because the derivative of $|\mathbf{\Lambda_\theta'Z'Z\Lambda_\theta+I}|$ with respect to elements of $\mathbf{\theta}$ essentially requires evaluating $\left(\mathbf{\Lambda_\theta'Z'Z\Lambda_\theta+I}\right)^{-1}$
- At least I haven't found a way of evaluating the gradient without the moral equivalent of evaluating the inverse.

# Summary

## Implementations in _lme4_ and in _MixedModels_

- Important concepts
    - relative covariance factor, $\mathbf{\Lambda_\theta}$, where $\mathbf{\Sigma}=\sigma^2\mathbf{\Lambda_\theta\Lambda_\theta'}$
    - linear predictor $\mathbf{\eta}=\mathbf{X\beta+Z\Lambda_\theta u}$
    - sparse Cholesky factor, $\mathbf{L}$, where $\mathbf{LL'=P}\left(\mathbf{\Lambda_\theta'Z'Z\Lambda_\theta+I}\right)\mathbf{P}'$
- For LMMs optimize the _profiled deviance_, $$-2\ell(\mathbf{\theta|y})=\log\left(|\mathbf{L_\theta}|^2\right)+n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right]$$

- For GLMMs the PLS problem becomes PIRLS and we use the Laplace approximation.
    - optimization problem is more difficult (both $\mathbf{\theta}$ and $\mathbf\beta$)
    - objective function evaluation is iterative, hence optimizing a stochastic function evaluation
