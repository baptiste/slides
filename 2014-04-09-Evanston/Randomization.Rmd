---
title: "Randomization tests"
author: "Douglas Bates"
date: "04/03/2014"
output: html_document
---
```{r preliminaries,echo=FALSE,include=FALSE}
library(lattice)
```

## Purpose

In the interests of full disclosure, the purpose of this contribution to [Rpubs](http://rpubs.com) is to present code for a couple of simple tests as implemented in R so they can be compared to [Julia](http://julialang.org) code that I will publish in a gist.

# A randomization test in paired designs

One of the easiest types of statistical tests to explain is a [randomization test](http://en.wikipedia.org/wiki/Randomization_test) comparing two samples of responses.  In a paired design we reduce the two, equally-sized sets of responses to their differences.  That is, if we write the two samples as $(x_i,y_i), i=1,\dots,n$ then we consider the differences $d_i = y_i - x_i, i=1,\dots,n$.

Under the hypothesis $H_0:\mu_X=\mu_Y$ that the means of the two populations are equal the signs of the individual differences are arbitrary.  Under the alternative, say $H_a:\mu_X < \mu_Y$ we expect the differences to be, on average, positive.

We can compare the sum (or, equivalently, the average) of the observed differences to the set of all possible differences that we could generate under $H_0$ by flipping the signs of some of the differences.  If we have $k$ differences there will be $2^k$ possible sums corresponding to changes of sign on some subset of the differences.

## Sample data from a paired design

The [Secchi Disk](en.wikipedia.org/wiki/Secchi_disk) is used to measure water transparency in lakes or oceans.  The measurements are conducted by lowering the disk into the water and recording the depth at which the disk is no longer visible.  Such measurements were conducted in 1980 on 22 Wisconsin lakes and in 1990 on the same lakes.  If the water clarity (or, conversely, the turbidity) remained constant than the differences in the Secchi depths would be neither systematically positive nor negative.

```{r}
Secchi <- within(
    data.frame(d80=c(2.11,1.79,2.71,1.89,1.69,1.71,2.01,1.36,2.08,1.10,1.29,
                     2.11,2.47,1.67,1.78,1.68,1.47,1.67,2.31,1.76,1.58,2.55),
               d90=c(3.67,1.72,3.46,2.60,2.03,2.10,3.01,1.82,2.64,2.23,1.39,
                     2.08,2.92,1.90,2.44,2.23,2.43,1.91,3.06,2.26,1.48,2.35)),
    diff <- d90 - d80)
str(Secchi)
sum(Secchi$diff)
```

As we are primarily interested in the differences we check the distribution of the differences with an empirical density plot.
```{r dotplot,fig.width=8,fig.height=3,echo=FALSE}
densityplot(~diff, Secchi, 
       xlab="Difference between 1990 and 1980 Secchi depths on 22 Wisconsin lakes")
```

It seems that the differences are systematically positive, which is good news because it means that the lakes were, on average, cleaner in 1990 than in 1980.

## Reference distribution of signed sums

To conduct the test we want the distribution of the possible sums resulting from arbitrary changes in the signs on the differences.  There are $2^{22}$ or `r 2^22` such sums.  Enumerating all of them is not easy in R.  Certainly it would be a mistake to try to do so by creating 22 nested loops!

However, there are ways of generating all the possible combinations of -1 and +1, say by using `expand.grid`.  For 3 differences it would look like
```{r expandgrid}
(gg <- expand.grid(c(-1,1),c(-1,1),c(-1,1)))
(mm <- Secchi$diff[1:3] * t(data.matrix(gg)))
colSums(mm)
```

For 22 differences, writing the vectors to expand will get rather tedious but we can use the `do.call` and `lapply` functions to generate the call to `expand.grid`.  The reference distribution then becomes
```{r ref,cache=TRUE}
system.time(
    refd <- colSums(Secchi$diff * 
                        t(data.matrix(
                            do.call(expand.grid,lapply(1:22, function(i) c(-1,1))
                                    )))))
gc()
```
The observed reference density is, not coincidentally, very like that of a "normal" or Gaussian distribution.
```{r refdens,echo=FALSE,fig.width=10,fig.height=4,cache=TRUE}
densityplot(~refd, plot.points=FALSE,xlab="Reference distribution of signed sums",ref=TRUE)
```

The _p-value_ for a test of $H_0:\mu_{80}=\mu_{90}$ versus $H_a:\mu_{80}<\mu{90}$ is
```{r p-value}
sum(refd >= sum(Secchi$diff))/length(refd)
```

## Sampling from the reference distribution

A more common approach to working with the reference distribution is to generate a reasonably large random sample from the distribution.  Generation of one instance of a sum with randomly allocated signs is often written using a random sample from a binomial distribution with size 1 and probability of success, 0.5
```{r rbinom}
set.seed(1234321)
rbinom(22,1,0.5)
```
but it is somewhat cleaner to simply generate a sample from a uniform (0,1) distribution and compare them to 0.5
```{r runif}
set.seed(1234321)
c(-1,1)[1 + (runif(22) > 0.5)]
```
We can vectorize this calculation to obtain, say, 100,000 samples from the distribution
```{r refsamp}
ns <- 100000L
system.time(refsamp <- colSums(Secchi$diff * matrix(c(-1,1)[1 + (runif(22*ns)>0.5)],nrow=22)))
```
with the corresponding density
```{r refdens1,echo=FALSE,fig.width=10,fig.height=4,cache=TRUE}
densityplot(~refsamp, plot.points=FALSE,xlab="Sampled Reference distribution of signed sums",ref=TRUE)
```
and _p-value_
```{r p-value2}
sum(refsamp >= sum(Secchi$diff))/length(refsamp)
```
Notice that a sample of size 100,000 may not be large enough to evaluate this very small _p-value_ accurately.

# Randomization tests for independent samples

If we have collected responses under two different conditions and the allocation of conditions has been randomized then a test of, say, $H_0:\mu_1=\mu_2$ versus $H_a:\mu_1<\mu_2$ can be based upon the set of possible combinations of response values.  Under the null hypothesis, the sample mean of the particular combination of responses that we saw for the first condition should be similar to the other possible sample means of combinations of responses of this size.  Under the alternative hypothesis the sample mean from the combination of responses we saw should be systematically smaller than the other possible sample means.

In the first chapter of Bob Wardrop's [course notes for Statistics 371](http://www.stat.wisc.edu/~wardrop/courses/371chapter1fall2013a.pdf) he describes an experiment performed by a student on the consumption of treats by her cat according to whether the treats are chicken or tuna flavored.  The experiment lasted for 20 days. The assignment of chicken or tuna on each day was randomized, subject to the constraint that each flavor is provided exactly 10 times.

Each day 10 treats of the indicated flavor were provided and the number consumed was recorded
```{r treats}
treats <- c(4,3,5,0,5,4,5,6,1,7,6,3,7,1,3,6,3,5,1,2)
```
The tuna treats were provided on days
```{r tuna}
tuna <- c(2,3,4,6,10,12,14,17,19,20)
```
Thus the number of tuna treats consumed (out of a possible 100) was
```{r sumtuna}
sum(treats[tuna])
```
The number of chicken treats consumed, also out of a possible 100, was
```{r sumchicken}
sum(treats) - sum(treats[tuna])
```

## The test
We want to compare the observed sum, 29, to the possible sums of any 10 of the 20 days.  It is not easy to enumerate the possible sums.  I don't know of a good way in R of stepping through all the
```{r choose2010}
choose(20L,10L)
```
possible selections.

The alternative is to produce a random sample from the reference distribution.  About the best way I can think of doing this is
```{r replicate}
system.time(tunaref <- replicate(100000,sum(sample(treats,10L))))
```
with the histogram
```{r hist,echo=TRUE,fig.width=10,fig.height=6,echo=FALSE}
histogram(~tunaref,breaks=seq(min(tunaref)-0.5, max(tunaref)+0.5,by=1.0),xlab="Sample from the reference distribution of sums")
```
and the _p-value_ of
```{r pvalue3}
sum(tunaref <= 29)/length(tunaref)
```



